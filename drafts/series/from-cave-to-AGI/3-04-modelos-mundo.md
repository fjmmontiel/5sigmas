# Modelos del mundo: JEPA, Sora y el futuro

Los LLMs son increíbles, pero tienen un defecto fundamental: aprenden del texto, no de la realidad física. Saben que "el fuego quema" porque lo han leído, no porque entiendan la física del calor.

## El debate: ¿Basta con el lenguaje?

Algunos (como OpenAI) creen que con suficientes datos y escala, el lenguaje es suficiente para entender el mundo. Modelos como **Sora** (generación de video) parecen confirmar esto: al predecir píxeles, Sora parece "entender" la gravedad, la oclusión y la persistencia de objetos. OpenAI lo llama un "Simulador de Mundos".

## Yann LeCun y JEPA

Otros, liderados por **Yann LeCun** (Meta), dicen que esto es un callejón sin salida. Generar cada píxel es ineficiente y propenso a alucinaciones.

LeCun propone **JEPA (Joint Embedding Predictive Architecture)**. En lugar de predecir píxeles, JEPA predice representaciones abstractas (conceptos) del futuro. La idea es que la IA aprenda un **Modelo del Mundo** interno: una comprensión de causa y efecto física, similar a la que tiene un gato o un bebé, antes de aprender lenguaje.

## Hacia la AGI

El camino hacia la Inteligencia Artificial General (AGI) probablemente requiera fusionar ambos mundos: la capacidad de razonamiento simbólico y lingüístico de los LLMs (Sistema 2) con la intuición física y el sentido común de los Modelos del Mundo (Sistema 1).

## Referencias

- [Yann LeCun: A Path Towards Autonomous Machine Intelligence](https://openreview.net/pdf?id=BZ5a1r-kVsf)
- [Video Generation Models as World Simulators (Sora)](https://openai.com/research/video-generation-models-as-world-simulators)
- [Introduction to Joint Embedding Predictive Architectures (JEPA)](https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/)

---

[Siguiente: Bonus - Scaling & Energy >](04-scaling-energy-limits.md){ .md-button .md-button--primary }
