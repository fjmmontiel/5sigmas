---
title: De las cavernas a la AGI
description: Recorrido histórico y conceptual desde símbolos hasta modelos actuales.
---
# De las cavernas a la AGI

{{ include_html("snippets/series_meta.html", series_dir="from-cave-to-agi", data_state="construction", data_level="general", data_read="7", status_label="En construcción", level_label="General", glow_hidden="true", extra_rows="") }}

Esta serie es un viaje histórico desde los primeros intentos de la humanidad por abstraer el mundo mediante símbolos hasta la creación de modelos de lenguaje masivos que *parecen entenderlo*.<br>

Exploraremos cómo la necesidad de contar llevó al cálculo, cómo el deseo de mecanizar el razonamiento llevó a la computación, y cómo el intento de imitar el cerebro nos trajo a la era de la IA generativa.<br>

## Índice
En esta serie exploraremos: <br>

### 1. Representar (≈ 40 000 a. C. – 1700)
- **Inventar lenguajes para describir el mundo:** del conteo físico (muescas, marcas) a números y símbolos manipulables.
- **Formalizar la verdad:** los griegos fijan demostración y geometría como estándar de conocimiento verificable.
- **Hacer ciencia con matemáticas:** notación (cero, álgebra) y modelo para la predicción. Nacimiento del cálculo para describir el concepto de cambio de magnitudes (derivadas, ecuaciones).<br>


### 2. Mecanizar (≈ 1700 – 1956)
- **Convertir símbolos en máquina:** automatizar cálculo con mecanismos y, luego, con circuitos. Aquí nace la obsesión por computar.
- **Separar programa de hardware:** de Jacquard/Babbage a la idea de instrucciones reutilizables independientes de la máquina.
- **Fundamentos de la computación moderna:** lógica (Boole), qué es “probar”, computabilidad y límites (Turing), arquitectura real (memoria), información/compresión (Shannon).<br>


### 3. Aprender (≈ 1956 – 2012)
- **De reglas a datos:** pasamos de codificar inteligencia a mano, a ajustar modelos con datos.
- **Motores del aprendizaje práctico:** probabilidad (Bayes, generalización), algoritmos de optimización (gradiente, regularización) y ajuste de parámetros (backprop) como tríada para el entrenamiento de IA.
- **Antes de la era LLM:** perceptrón y límites, auge/caída de IA simbólica, NLP estadístico (n-grams, HMM/CRF) y renacimiento neuronal impulsado por datos, GPUs, benchmarks y escala.<br>


### 4. Escalar (≈ 2012 – hoy)
- **Escala como multiplicador:** deep learning despega por más datos/cómputo y mejores representaciones. 2012 marca el cambio de régimen.
- **Transformers y modelos fundacionales:** la atención habilita preentrenamiento masivo y reutilización generalista. Las leyes de escalado y el alineamiento con el humano lo convierte en un punto de inflexión que deriva en el nacimiento de ChatGPT.
- **De “responder” a “actuar” y unificar modalidades:** herramientas/agentes, multimodalidad (texto/imagen/audio)
- **Proximos pasos**: modelos del mundo (simulación/planificación) y líneas post-Transformer (Titans, nested learning, etc.).<br>
