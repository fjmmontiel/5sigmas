# De las cavernas a la AGI

Esta serie es un viaje histórico desde los primeros intentos de la humanidad por abstraer el mundo mediante símbolos hasta la creación de modelos de lenguaje masivos que parecen "entenderlo".

Exploraremos cómo la necesidad de contar llevó al cálculo, cómo el deseo de mecanizar el razonamiento llevó a la computación, y cómo el intento de imitar el cerebro nos trajo a la era de la IA generativa.

<!-- ## Escucha el podcast de la serie

{{ podcast_player('from-cave-to-AGI') }} -->
<!-- 
## Vídeos

{{ video_gallery('from-cave-to-AGI') }} -->
## Contenido

### Acto I: Representar (≈ 40 000 a. C. – 1700)
Cómo nacen los lenguajes formales con los que describimos el mundo.

### Acto II: Mecanizar (≈ 1700 – 1956)
Cómo pasamos de aprender a representar el mundo con símbolos a construir máquinas que operan símbolos.


### Acto III: Aprender (≈ 1956 – 2012)
Cómo aparece la idea central moderna: no programar reglas, sino **ajustar modelos con datos**.

### Acto IV: Escalar (2012 – hoy)
Cómo el mismo principio aprendizaje + escala cruza umbrales y aparecen modelos “fundacionales”. Y la explosión de posibilidades que abre esta nueva tecnología

<!-- ### Acto I: Representar (≈ 40 000 a. C. – 1700)
Cómo nacen los lenguajes formales con los que describimos el mundo.

- De las muescas en hueso al número: cuándo contar se vuelve abstracción
- El proyecto griego: demostración, geometría y verdad formal
- Cero y notación: el truco que hace posible el álgebra
- Álgebra como lenguaje: de recetas a símbolos manipulables
- La idea de ley: medir, modelar, predecir (y por qué eso es “hacer ciencia”)
- Del cálculo al mundo dinámico: cambio, derivadas y ecuaciones

---

### Acto II: Mecanizar (≈ 1700 – 1956)
Cómo pasamos de “razonar con símbolos” a “construir máquinas que operan símbolos”.

- Mecanizar el cálculo: autómatas, engranajes y la obsesión por computar
- Babbage, Jacquard y la idea de programa: instrucciones separadas de la máquina
- Lógica como ingeniería: de Boole a circuitos
- Crisis de fundamentos: qué significa “probar” y por qué importa
- Máquina universal: Turing, computabilidad y límites
- Arquitectura y memoria: de la teoría al ordenador real
- Información y compresión: por qué Shannon es una bisagra (y qué compra para IA)

---

### Acto III: Aprender (≈ 1956 – 2012)
Cómo aparece la idea central moderna: no programar reglas, sino **ajustar modelos con datos**.

- Dartmouth y el sueño inicial: “hacer inteligencia” por definición
- Primeras redes: neuronas de metal, Perceptrón y el primer choque con los límites
- IA simbólica y sistemas expertos: cuando las reglas dominan (y por qué fallan)
- Probabilidad en la práctica: Bayes, verosimilitud y generalización
- Optimización como motor: gradiente, regularización y el arte de entrenar
- NLP antes de los LLMs: n-grams, HMM/CRF y por qué funcionaban
- El renacimiento neuronal: backprop, representaciones y el retorno del aprendizaje
- Datos + hardware: el papel real de GPUs, benchmarks y escala industrial

---

### Acto IV: Escalar (≈ 2012 – hoy)
Cómo el mismo principio aprendizaje + escala cruza umbrales y aparecen modelos “fundacionales”.

- 2012 como punto de inflexión: deep learning “gana” por escala y representación
- Atención y Transformers: el cambio de arquitectura que desbloquea preentrenamiento masivo
- Preentrenar para todo: por qué “foundation model” no es solo marketing
- Leyes de escalado y límites
- Instrucción y alineamiento
- Herramientas y agentes: cuando el modelo deja de solo “responder” y empieza a actuar
- Multimodalidad: texto+imagen+audio como unificación práctica
- Modelos del mundo: representación, simulación y planificación (qué significa realmente)
- Qué viene después del Transformer: memoria larga, eficiencia, modularidad, nuevas familias -->

---
